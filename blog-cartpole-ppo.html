<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Training CartPole Agent with PPO - eigenron</title>
  <link rel="stylesheet" href="style.css">
  <style>
    .blog-header {
      max-width: 700px;
      margin-bottom: 40px;
    }
    
    .blog-title-main {
      font-size: 32px;
      font-weight: 600;
      margin-bottom: 10px;
      color: var(--text-primary);
    }
    
    .blog-meta {
      color: var(--text-secondary);
      font-size: 14px;
      margin-bottom: 10px;
    }
    
    .back-link {
      color: var(--link-color);
      text-decoration: none;
      font-size: 14px;
      display: inline-block;
      margin-bottom: 20px;
    }
    
    .back-link:hover {
      text-decoration: underline;
    }
    
    .blog-content {
      max-width: 700px;
    }
    
    .blog-content h2 {
      font-size: 24px;
      font-weight: 600;
      margin-top: 40px;
      margin-bottom: 15px;
      color: var(--text-primary);
    }
    
    .blog-content h3 {
      font-size: 20px;
      font-weight: 600;
      margin-top: 30px;
      margin-bottom: 12px;
      color: var(--text-primary);
    }
    
    .blog-content p {
      margin-bottom: 20px;
      font-size: 16px;
      line-height: 1.7;
    }
    
    .blog-content code {
      background-color: var(--code-bg);
      padding: 2px 6px;
      border-radius: 3px;
      font-family: 'Courier New', monospace;
      font-size: 14px;
    }
    
    .blog-content pre {
      background-color: var(--code-bg);
      padding: 15px;
      border-radius: 5px;
      overflow-x: auto;
      margin-bottom: 20px;
    }
    
    .blog-content pre code {
      background-color: transparent;
      padding: 0;
    }
    
    .blog-content ul, .blog-content ol {
      margin-bottom: 20px;
      padding-left: 25px;
    }
    
    .blog-content li {
      margin-bottom: 8px;
      line-height: 1.6;
    }
    
    .blog-content img {
      max-width: 100%;
      height: auto;
      margin: 20px 0;
      border-radius: 5px;
    }
    
    .blog-content blockquote {
      border-left: 3px solid #007acc;
      padding-left: 20px;
      margin: 20px 0;
      color: var(--text-secondary);
      font-style: italic;
    }
    
    .blog-content figure {
      margin: 30px 0;
      text-align: center;
    }
    
    .blog-content figcaption {
      margin-top: 10px;
      font-size: 14px;
      color: var(--text-secondary);
      font-style: italic;
    }
    
    .blog-content video {
      max-width: 100%;
      height: auto;
      border-radius: 5px;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    }
    
    #references ol {
      list-style-type: none;
      counter-reset: ref-counter;
      padding-left: 0;
    }
    
    #references ol li {
      counter-increment: ref-counter;
      margin-bottom: 15px;
      padding-left: 35px;
      position: relative;
      line-height: 1.6;
    }
    
    #references ol li::before {
      content: "[" counter(ref-counter) "]";
      position: absolute;
      left: 0;
      font-weight: 600;
      color: var(--text-primary);
    }
    
    #references a {
      color: var(--link-color);
      text-decoration: none;
      word-break: break-all;
    }
    
    #references a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>
  <div class="container">
    <header>
      <h1 class="site-title">eigenron</h1>
      <nav>
        <a href="index.html" class="nav-link">home</a>
        <a href="blogs.html" class="nav-link active">blog</a>
        <a href="#" class="nav-link">projects</a>
        <a href="https://x.com/eigenron" class="nav-link" target="_blank">X</a>
        <a href="https://github.com/ronaldnetawat" class="nav-link" target="_blank">GitHub</a>
        <a href="#" class="nav-link">books</a>
        <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
          <svg class="theme-icon sun-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <circle cx="12" cy="12" r="5"/>
            <path d="M12 1v6m0 6v6m8.66-7H15m-6 0H3m15.66 7.66l-4.24-4.24m-4.24 0L5.34 4.34m13.32 15.32l-4.24-4.24m-4.24 0L5.34 19.66"/>
          </svg>
          <svg class="theme-icon moon-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
          </svg>
        </button>
      </nav>
    </header>
    
    <main>
      <a href="blogs.html" class="back-link">← back to blog</a>
      
      <article>
        <div class="blog-header">
          <h1 class="blog-title-main">Training a CartPole Agent with Proximal Policy Optimization (PPO)</h1>
          <div class="blog-meta">October 4, 2025</div>
        </div>
        
        <div class="blog-content">
          
            <section id="intro">
              <h2>1. Introduction</h2>
              <p>
                Reinforcement Learning (RL) has become one of the most exciting areas of machine learning,
                with applications ranging from robotics to games like Go and StarCraft. A classic benchmark
                for testing RL algorithms is the <strong>CartPole environment</strong> [3], where an agent must
                balance a pole on a moving cart by applying forces left or right.
              </p>
              <p>
                In this blog, we'll walk through training a CartPole agent using
                <strong>Proximal Policy Optimization (PPO)</strong> [1] — one of the most popular and effective
                policy gradient algorithms. We'll cover the mathematical background, explain why PPO works,
                and then dive into the implementation details from the code.
              </p>
            </section>
          
            <section id="rl-basics">
              <h2>2. Reinforcement Learning Basics</h2>
              <p>
                At its core, RL is modeled as a <strong>Markov Decision Process (MDP)</strong>, defined by:
              </p>
              <ul>
                <li><strong>States</strong> \(s_t\): the environment’s representation (Cart position, velocity, pole angle, etc.).</li>
                <li><strong>Actions</strong> \(a_t\): discrete choices (push cart left or right).</li>
                <li><strong>Rewards</strong> \(r_t\): scalar feedback (here +1 for every timestep the pole stays upright).</li>
              </ul>
              <p>
                The agent follows a <strong>policy</strong> \(\pi(a|s)\), mapping states to action probabilities.
                In <em>policy gradient methods</em>, we directly optimize this policy by maximizing the
                expected cumulative reward:
              </p>
              <p class="math">
                \[
                J(\theta) = \mathbb{E}_{\pi_\theta} \Bigg[ \sum_t \gamma^t r_t \Bigg]
                \]
              </p>
            </section>
          
            <section id="trpo">
              <h2>3. Background: TRPO</h2>
              <p>
                Vanilla policy gradient suffers from <strong>high variance</strong> and <strong>unstable updates</strong>.
                Trust Region Policy Optimization (TRPO) [2] addressed this by ensuring each update stays close to
                the old policy using a KL-divergence constraint:
              </p>
              <p class="math">
                \[
                \max_\theta \; \mathbb{E}\left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)} \hat{A}_t \right]
                \quad \text{s.t.} \quad
                D_{KL}(\pi_{\theta_{old}} \;||\; \pi_\theta) \leq \delta
                \]
              </p>
              <p>
                TRPO guarantees monotonic improvement, but requires expensive second-order optimization.
                This makes it impractical for large-scale problems.
              </p>
            </section>
          
            <section id="ppo">
              <h2>4. PPO: The Key Idea</h2>
              <p>
                PPO simplifies TRPO by replacing the hard KL constraint with a <strong>clipped surrogate objective</strong>.
                Instead of enforcing a strict trust region, it limits how much the probability ratio can change:
              </p>
              <p class="math">
                \[
                L^{CLIP}(\theta) = \mathbb{E}\Big[ \min\big(r_t(\theta)\hat{A}_t,\;\;
                \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t \big) \Big]
                \]
              </p>
              <p>
                where \(r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\) is the importance sampling ratio.
              </p>
              <p>
                PPO also adds:
              </p>
              <ul>
                <li><strong>Value loss</strong> for better baseline estimation.</li>
                <li><strong>Entropy bonus</strong> to encourage exploration.</li>
              </ul>
              <p>
                This combination makes PPO both <em>stable</em> and <em>sample-efficient</em>, which is why it has become the
                de facto standard in RL research and applications.
              </p>
            </section>
          
            <section id="math">
              <h2>5. Math Foundations</h2>
              <p>
                Let’s summarize the full PPO objective used in practice:
              </p>
              <p class="math">
                \[
                L(\theta) = \mathbb{E}\Big[ L^{CLIP}(\theta) - c_1 (V_\theta(s_t) - R_t)^2 + c_2 H[\pi_\theta](s_t) \Big]
                \]
              </p>
              <ul>
                <li>\(L^{CLIP}\): clipped surrogate policy loss.</li>
                <li>\(V_\theta(s_t)\): value function prediction.</li>
                <li>\(R_t\): discounted return.</li>
                <li>\(H[\pi_\theta]\): policy entropy (encourages exploration).</li>
              </ul>
              <p>
                For advantage estimation, one can use <strong>Generalized Advantage Estimation (GAE)</strong>, which trades
                off bias and variance. Our implementation uses a simpler reward-to-go approach.
              </p>
            </section>
          
            <section id="cartpole">
              <h2>6. CartPole Setup</h2>
              <p>
                The CartPole environment (<code>CartPole-v1</code>) has:
              </p>
              <ul>
                <li><strong>Observations</strong>: [cart position, velocity, pole angle, angular velocity] (4 floats).</li>
                <li><strong>Actions</strong>: {0: push left, 1: push right}.</li>
                <li><strong>Reward</strong>: +1 for every timestep the pole remains balanced.</li>
                <li><strong>Termination</strong>: pole falls too far or cart moves out of bounds.</li>
              </ul>
              <p>
                It’s a simple yet effective testbed for experimenting with RL algorithms.
              </p>
            </section>
          
            <section id="implementation">
              <h2>7. Implementation Walkthrough</h2>
              <h3>Policy & Value Networks</h3>
              <p>
                We use a shared neural network with two heads:
              </p>
              <ul>
                <li><strong>Policy head</strong>: outputs action probabilities via softmax.</li>
                <li><strong>Value head</strong>: outputs a scalar estimate of state value.</li>
              </ul>
          
          <pre><code class="python">
          class ActorCritic(nn.Module):
              def __init__(self, state_dim, action_dim):
                  super().__init__()
                  self.shared = nn.Sequential(
                      nn.Linear(state_dim, 64), nn.ReLU(),
                      nn.Linear(64, 64), nn.ReLU()
                  )
                  self.policy = nn.Linear(64, action_dim)
                  self.value = nn.Linear(64, 1)
          
              def forward(self, x):
                  x = self.shared(x)
                  return F.softmax(self.policy(x), dim=-1), self.value(x)
          </code></pre>
          
              <h3>Rollout Collection & Batching</h3>
              <p>
                The agent collects <code>rollout_len</code> steps of experience, storing states, actions,
                rewards, and log-probabilities. After a rollout, we compute discounted returns:
              </p>
          
          <pre><code class="python">
          def compute_returns(rewards, dones, gamma, next_value):
              returns = []
              R = next_value
              for reward, done in zip(reversed(rewards), reversed(dones)):
                  if done: R = 0
                  R = reward + gamma * R
                  returns.insert(0, R)
              return returns
          </code></pre>
          
              <h3>Loss Functions</h3>
              <ul>
                <li><strong>Policy loss</strong>: PPO clipped surrogate objective.</li>
                <li><strong>Value loss</strong>: MSE between predicted and actual returns.</li>
                <li><strong>Entropy loss</strong>: encourages exploration.</li>
              </ul>
          
              <h3>Training Loop</h3>
              <p>
                We iterate over episodes, collect rollouts, split into minibatches, and optimize with Adam.
                Hyperparameters include:
              </p>
              <ul>
                <li>\(\gamma = 0.99\) (discount factor)</li>
                <li>\(\epsilon = 0.2\) (clip parameter)</li>
                <li>Learning rate = 3e-4</li>
                <li>Rollout length = 2048</li>
                <li>Batch epochs = 10</li>
              </ul>
            </section>
          
            <section id="results">
              <h2>8. Results</h2>
              <p>
                After training, our PPO agent achieves the maximum score: <strong>500 reward across 5 test runs</strong>.
                This means the agent successfully balances the pole for the full 500 steps per episode.
              </p>
              <figure>
                <video src="cartpole_trained_agent.mp4" controls loop muted></video>
                <figcaption>Trained PPO agent balancing the pole for 500 steps.</figcaption>
              </figure>
            </section>
          
            <section id="references">
              <h2>9. References</h2>
              <ol>
                <li>
                  Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). 
                  <em>Proximal Policy Optimization Algorithms</em>. 
                  arXiv preprint arXiv:1707.06347. 
                  <a href="https://arxiv.org/abs/1707.06347" target="_blank" rel="noopener">https://arxiv.org/abs/1707.06347</a>
                </li>
                <li>
                  Schulman, J., Levine, S., Abbeel, P., Jordan, M., & Moritz, P. (2015). 
                  <em>Trust Region Policy Optimization</em>. 
                  In International Conference on Machine Learning (pp. 1889-1897). PMLR. 
                  <a href="https://arxiv.org/abs/1502.05477" target="_blank" rel="noopener">https://arxiv.org/abs/1502.05477</a>
                </li>
                <li>
                  Towers, M., Terry, J. K., Kwiatkowski, A., Balis, J. U., Cola, G. d., Deleu, T., Goulão, M., Kallinteris, A., KG, A., Krimmel, M., Perez-Vicente, R., Pierré, A., Schulhoff, S., Tai, J. J., Shen, A. T. J., & Younis, O. G. (2023). 
                  <em>Gymnasium</em>. 
                  Zenodo. 
                  <a href="https://zenodo.org/record/8127025" target="_blank" rel="noopener">https://zenodo.org/record/8127025</a>
                </li>
              </ol>
            </section>
          
            <section id="conclusion">
              <h2>10. Conclusion</h2>
              <p>
                PPO combines the stability of TRPO with the simplicity of vanilla policy gradient methods,
                making it a powerful algorithm for continuous control problems. Even on a small benchmark
                like CartPole, we can see how PPO ensures stable and reliable training.
              </p>
            </section>
        </div>
      </article>
    </main>
  </div>

  <!-- MathJax for rendering equations -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <script>
    // Theme Toggle Functionality
    (function() {
      const themeToggle = document.getElementById('themeToggle');
      const body = document.body;
      
      // Check for saved theme preference or default to light mode
      const currentTheme = localStorage.getItem('theme') || 'light';
      if (currentTheme === 'dark') {
        body.classList.add('dark-mode');
      }
      
      // Toggle theme on button click
      themeToggle.addEventListener('click', () => {
        body.classList.toggle('dark-mode');
        
        // Save theme preference
        const theme = body.classList.contains('dark-mode') ? 'dark' : 'light';
        localStorage.setItem('theme', theme);
      });
    })();
  </script>
</body>
</html>

